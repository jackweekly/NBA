Metadata-Version: 2.4
Name: nbapredictor
Version: 0.1.0
Summary: Utilities for synchronising NBA data with the wyattowalsh/nbadb dataset.
Author: AutoGenerated
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.31
Requires-Dist: pandas>=2.0
Requires-Dist: nba_api>=1.5.7
Requires-Dist: python-dateutil>=2.9
Requires-Dist: pyyaml>=6.0
Requires-Dist: duckdb>=1.0
Requires-Dist: tenacity>=8.2
Requires-Dist: kaggle

# NBA Predictor Data Pipeline

This project provides a lightweight data ingestion layer for a future NBA prediction
system. The helper utilities mirror the behaviour of
[wyattowalsh/nbadb](https://github.com/wyattowalsh/nbadb)'s `nba_api_ingestion.py`
script so you can bulk-download the canonical CSV files and keep them fresh with
incremental updates.

## Features

* Resolves the raw data directory from `config.yaml`, matching the upstream
  repository layout.
* Pulls player, team, league game log and game summary tables directly from
  the `nba_api` endpoints.
* Provides a one-shot bootstrap that reproduces the historical CSVs published
  in the Kaggle dataset (without needing Kaggle credentials).
* Supports incremental backfills by date as well as full historical refreshes
  via season-at-a-time downloads.

## Installation

```bash
pip install -e .
```

The package depends on `requests`, `pandas`, `nba_api`, and `python-dateutil`.

### Kaggle CLI Setup

To enable the automatic download of the Kaggle bootstrap dataset, you need to install the Kaggle CLI and configure your credentials:

1.  Install the Kaggle package:
    ```bash
    pip install kaggle
    ```
2.  Set up your Kaggle API credentials:
    *   Go to your Kaggle account page (`https://www.kaggle.com/<username>/account`).
    *   Click on "Create New API Token" to download `kaggle.json`.
    *   Move this file to `~/.kaggle/kaggle.json` (create the `.kaggle` directory if it doesn't exist).
    *   Ensure the file has appropriate permissions (e.g., `chmod 600 ~/.kaggle/kaggle.json`).


## Usage

The primary entry point for running the data pipeline is `scripts/run_pipeline.py`.
This script handles both the one-shot bootstrap of historical data and daily
incremental updates.

The utilities look for `config.yaml` at the project root. The default file in
this repository points `raw.raw_dir` to `data/raw`, matching the original repo.

### Running the Pipeline

To run the full pipeline, including initial data seeding and daily updates:

```bash
python scripts/run_pipeline.py
```

This command will:
1. Ensure the Kaggle dataset is present (downloading it via `run_init.py` if needed).
2. Seed the DuckDB database using `scripts/seed_duckdb.py`.
3. Resolve home/away overrides via `scripts/fetch_home_away_overrides.py`.
4. Apply warehouse schema views using `scripts/apply_schema.py`.
5. Run quality checks with `scripts/check_quality.py`.
6. Perform daily incremental updates via `run_daily_update.py`.
7. Re-resolve home/away overrides and run quality checks again.

### Options

The `run_pipeline.py` script accepts the following arguments:

*   `--force-kaggle`: Force re-download of the Kaggle bootstrap dataset before seeding.
    ```bash
    python scripts/run_pipeline.py --force-kaggle
    ```
*   `--skip-daily`: Skip the daily incremental update (useful for bootstrapping only).
    ```bash
    python scripts/run_pipeline.py --skip-daily
    ```
*   `--offline-only`: Run home/away override resolution without network calls.
    ```bash
    python scripts/run_pipeline.py --offline-only
    ```
*   `--verbose`: Enable DEBUG logging for extra insight.
    ```bash
    python scripts/run_pipeline.py --verbose
    ```

## Testing

The repository contains a small unit test suite. Execute it with `pytest`:

```bash
pytest
```
